{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Proposal Components\n",
    "\n",
    "### 1. Data Description\n",
    "In the code cell below, use [Gymnasium](https://gymnasium.farama.org/) to set up a [Frozen Lake maze](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) for your project. When you are done with the set up, describe the reward system you plan on using.\n",
    "\n",
    "*Note, a level 5 maze is at least 10 x 10 cells large and contains at least five lake cells.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "cellTypes = \"SFFHFHFFFFFHFFFHFFFHFHFFHFHHFFFFFHHFFFFHHFHFFFHHFFFFFFFFHFFFHFFHFFHHFHFFFHFFFHFFFFHFFHHFFHFFHHFFHFFG\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = [\"SFFHFHFFFF\",\"FHFFFHFFFH\", \"FHFFHFHHFF\", \"FFFHHFFFFH\",\"HFHFFFHHFF\",\"FFFFFFHFFF\",\"HFFHFFHHFH\", \"FFFHFFFHFF\", \"FFHFFHHFFH\", \"FFHHFFHFFG\"]\n",
    "maze = [list(row) for row in maze]\n",
    "env = gym.make('FrozenLake-v1', desc=maze, render_mode='rgb_array', is_slippery=False)\n",
    "\n",
    "numStates = env.observation_space.n\n",
    "numActions = env.action_space.n\n",
    "Q = {state: [0] * numActions for state in range(numStates)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReward(state):\n",
    "    row = state // len(maze[0])\n",
    "    col = state % len(maze[0])\n",
    "    cell = maze[row][col]  # Get char POS\n",
    "    \n",
    "    if cell == \"G\":\n",
    "        return 100\n",
    "    elif cell == \"H\":\n",
    "        return -100\n",
    "    else:\n",
    "        return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateQTable(q, alpha, gamma, state, next_state, action):\n",
    "    current_q = q[state][action]\n",
    "    reward = getReward(next_state)\n",
    "\n",
    "    # Make sure next state has valid Q values\n",
    "    if next_state in q:\n",
    "        next_max_q = max(q[next_state])  \n",
    "    else:\n",
    "        next_max_q = 0  # Default to 0 if the next state is unknown\n",
    "\n",
    "    # Update formula\n",
    "    new_q = (1 - alpha) * current_q + alpha * (reward + gamma * next_max_q)\n",
    "    \n",
    "    q[state][action] = new_q  # Store updated Q value\n",
    "    \n",
    "    return new_q  # Return for debugging this mess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random ##We're going to use Random to choose random paths at the start. \n",
    "\n",
    "def chooseAction(q, state):\n",
    "    \"\"\"Select best action but add a small chance to explore other actions.\"\"\"\n",
    "    if random.random() < 0.1:  # 10% chance to explore a random action\n",
    "        return random.randint(0, 3)\n",
    "    \n",
    "    return q[state].index(max(q[state]))  # Pick the highest Q value action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The reward system is as follows: Goal = +100,000, Frozen Lake = -1, Hole = -100,000,000. This will incentivize finding the most optimal path towards a goal. However, if an agent enters into a hole, said agent will be nuked from existence.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training Your Model\n",
    "In the cell seen below, write the code you need to train a Q-Learning model. Display your final Q-table once you are done training your model.\n",
    "\n",
    "*Note, level 5 work uses only the standard Python library and Pandas to train your Q-Learning model. A level 4 uses external libraries like Baseline3.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "terminated = False\n",
    "\n",
    "for episode in range(10000):\n",
    "    current_state, _ = env.reset()\n",
    "    terminated = False\n",
    "    \n",
    "    while not terminated:\n",
    "        action = chooseAction(Q, current_state)  # Select best action\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        new_q = updateQTable(Q, alpha, gamma, current_state, new_state, action)\n",
    "\n",
    "        print(f\"Episode {episode}, State {current_state}, Action {action}, New Q: {new_q}\")\n",
    "\n",
    "        current_state = new_state\n",
    "\n",
    "\n",
    "data = pd.DataFrame.from_dict(Q, orient=\"index\", columns=[\"Left\", \"Down\", \"Right\", \"Up\"])\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame.from_dict(Q, orient=\"index\", columns=[\"Left\", \"Down\", \"Right\", \"Up\"])\n",
    "print(data.head())  # Show Q-table\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Testing Your Model\n",
    "In the cell seen below, write the code you need to test your Q-Learning model for **1000 episodes**. It is important to test your model for 1000 episodes so that we are all able to compare our results.\n",
    "\n",
    "*Note, level 5 testing uses both a success rate and an average steps taken metric to evaluate your model. Level 4 uses one or the other.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize test environment\n",
    "envTest = gym.make('FrozenLake-v1', desc=maze, render_mode='human', is_slippery=False)\n",
    "current_state, _ = envTest.reset()\n",
    "terminated = False\n",
    "\n",
    "while not terminated:\n",
    "    action = chooseAction(Q, current_state)  # Select best action\n",
    "    new_state, reward, terminated, truncated, info = envTest.step(action)\n",
    "    envTest.render()\n",
    "    current_state = new_state\n",
    "\n",
    "envTest.close()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
